{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-57d7fc784c26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtslearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtslearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclustering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeSeriesKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tslearn/clustering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \"\"\"\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClusterMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpairwise_kernels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/class_weight.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcontextlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_get_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNonBLASDotWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPositiveSpectrumWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlsqr\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msparse_lsqr\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaskedArray\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_MaskedArray\u001b[0m  \u001b[0;31m# TODO: remove in 0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/scipy/stats/distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from ._distn_infrastructure import (entropy, rv_discrete, rv_continuous,\n\u001b[0m\u001b[1;32m     11\u001b[0m                                     rv_frozen)\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mintegrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# to approximate the pdf of a continuous distribution given its cdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/scipy/integrate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0modepack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquadpack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_ode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bvp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msolve_bvp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m from ._ivp import (solve_ivp, OdeSolution, DenseOutput,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/scipy/integrate/_ode.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mdopri5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIntegratorBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m     \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dopri5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dopri5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tslearn\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import davies_bouldin_score, pairwise_distances\n",
    "from gap_statistic import OptimalK\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trimmed = pd.read_csv('Data/data_trimmed.csv') #return data\n",
    "sp500 = pd.read_csv('Data/sp500.csv') #sp500 return data\n",
    "ticker_data = pd.read_csv('Data/Tickers.csv') #fundno-ticker\n",
    "nameticker = pd.read_excel('Data/nameticker.xlsx',sheet_name = 'Category') #ticker-name-morningstar category\n",
    "holding_asset = pd.read_csv('Data/Summary_Updated.csv') #holding_asset data\n",
    "holding_asset = holding_asset.iloc[:, [0, 2]+[i for i in range(17, 30)]]\n",
    "fund_mornstar = pd.read_csv('Data\\Summary_Updated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = {}\n",
    "ticker_fundno = {}\n",
    "fundno_ticker = {}\n",
    "\n",
    "for i in range(nameticker.shape[0]):\n",
    "    rec[nameticker.Name[i]] = nameticker.Ticker[i]\n",
    "\n",
    "for i in range(ticker_data.shape[0]):\n",
    "    if pd.isnull(ticker_data.ticker[i]):\n",
    "        continue\n",
    "    ticker_fundno[ticker_data.ticker[i]] = ticker_data.crsp_fundno[i]\n",
    "    fundno_ticker[ticker_data.crsp_fundno[i]] = ticker_data.ticker[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500.date = pd.to_datetime(sp500.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trimmed = data_trimmed.set_index('date')\n",
    "data_trimmed.index = pd.to_datetime(data_trimmed.index)\n",
    "data_trimmed = data_trimmed.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holding_asset['per_equity'] = holding_asset['per_com'] + holding_asset['per_pref'] + holding_asset['per_eq_oth']\n",
    "holding_asset['per_bond'] = holding_asset['per_conv'] + holding_asset['per_corp'] + holding_asset['per_muni'] + holding_asset['per_govt']\n",
    "holding_asset['per_sec'] = holding_asset['per_abs'] + holding_asset['per_mbs'] + holding_asset['per_fi_oth'] + holding_asset['per_oth']\n",
    "holding_asset.caldt = pd.to_datetime(holding_asset.caldt, format='%Y%m%d')\n",
    "holding_asset = holding_asset[['crsp_fundno', 'caldt', 'per_cash', 'per_equity', 'per_bond', 'per_sec']]\n",
    "holding_asset.columns = ['crsp_fundno', 'caldt', 'cash', 'equity', 'bond', 'security']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_mornstar = fund_mornstar[['crsp_fundno', 'caldt', 'lipper_class_name']]\n",
    "fund_mornstar.caldt = pd.to_datetime(fund_mornstar.caldt, format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(holding_asset.columns)[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering - First layer based on holding asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class first_layer_clustering:\n",
    "    def __init__(self, clustering_year, ret_data, asset_data, mrnstar_class_data):\n",
    "        self.clustering_year = clustering_year\n",
    "        self.ret_data = ret_data[ret_data.index.year == clustering_year]\n",
    "        self.cumret_data = (self.ret_data+1).cumprod()\n",
    "        self.asset_data = asset_data[(asset_data.caldt.dt.year == clustering_year) & (asset_data.caldt.dt.month == 12)]\n",
    "        self.asset_type = list(self.asset_data.columns)[2:]\n",
    "        self.mrnstar_data = mrnstar_class_data[(mrnstar_class_data.caldt.dt.year == (clustering_year)) & (mrnstar_class_data.caldt.dt.month == 12)]\n",
    "        self.feature = None\n",
    "        self.feature_nostd = None\n",
    "        self.label = None\n",
    "        self.k = None\n",
    "\n",
    "    def init_features(self):\n",
    "        \n",
    "        def standardization(df):\n",
    "            for column in df.columns:\n",
    "                df[column] = (df[column] - df[column].mean())/np.std(df[column])\n",
    "            return df\n",
    "        \n",
    "        feature = pd.DataFrame(index=self.ret_data.columns)\n",
    "        funds = set(self.asset_data.crsp_fundno.values)\n",
    "        \n",
    "        for index in feature.index:\n",
    "            if int(index) in funds:\n",
    "                for a in self.asset_type:A\n",
    "                    feature.loc[index, a] = self.asset_data[self.asset_data.crsp_fundno == int(index)][a].values[0]\n",
    "            else:\n",
    "                for a in self.asset_type:\n",
    "                    feature.loc[index, a] = np.NaN\n",
    "        \n",
    "        feature.dropna(axis=0, inplace=True)\n",
    "        self.feature = feature\n",
    "        self.ret_data = self.ret_data[self.feature.index]\n",
    "        self.cumret_data = self.cumret_data[self.feature.index]\n",
    "        \n",
    "        self.feature_nostd = self.feature.copy()\n",
    "        self.feature = standardization(self.feature)\n",
    "        self.feature = np.round(self.feature, 4)\n",
    "        return self.feature, self.feature_nostd\n",
    "    \n",
    "    def asset_based_clustering(self, log=False, identical_asset=3, argsort=False):\n",
    "        #Functions will be used\n",
    "        def get_silhouette(feature, log=False):\n",
    "            from sklearn import metrics\n",
    "            score = []\n",
    "            mx = 0\n",
    "            res = None\n",
    "            for i in range(2, 30):\n",
    "                k=i\n",
    "                clustering = cluster.AgglomerativeClustering(n_clusters=k).fit(feature)\n",
    "                cluster_label = clustering.labels_\n",
    "                score.append(metrics.silhouette_score(feature, cluster_label, metric='euclidean'))\n",
    "                if score[-1] >= mx:\n",
    "                    mx = score[-1]\n",
    "                    res = i\n",
    "            if log:\n",
    "                plt.plot(list(range(2, 30)), score)\n",
    "                plt.xlim(2, 30)\n",
    "                print('The best k determined by statistical approach is', res)\n",
    "            return res\n",
    "\n",
    "\n",
    "        def get_new_center(feature, label, k):\n",
    "            cluster_center_init = []\n",
    "            for i in range(k):\n",
    "                df = feature[label == i]\n",
    "                cur_cluster_center = df.median(axis=0)\n",
    "                if len(df) == 0:\n",
    "                    continue\n",
    "                sit = False\n",
    "                for ass in list(df.columns):\n",
    "                    ass_25 = np.percentile(df[ass], 25)\n",
    "                    ass_75 = np.percentile(df[ass], 75)\n",
    "                    low = max(ass_25 - 1.5*(ass_75-ass_25), df[ass].min())\n",
    "                    high = min(ass_75 + 1.5*(ass_75-ass_25), df[ass].max())\n",
    "                    \n",
    "                    if ass_75 - ass_25 > 20:\n",
    "                        cluster_center_init.append(df[df[ass] == np.sort(df[ass])[len(df[ass])*3//4]].iloc[0, :])\n",
    "                        cluster_center_init.append(df[df[ass] == np.sort(df[ass])[len(df[ass])*1//4]].iloc[0, :])\n",
    "                        sit = True\n",
    "                    elif high - low >= 50:\n",
    "                        cluster_center_init.append(df[df[ass] == np.sort(df[df[ass] >= high][ass])[sum(df[ass] >= high)//2]].iloc[0, :])\n",
    "                        cluster_center_init.append(df[df[ass] == np.sort(df[df[ass] <= low][ass])[sum(df[ass] <= low)//2]].iloc[0, :])\n",
    "                        sit = True\n",
    "                    elif high - ass_75 > 20 or sum(df[ass] >= high) >= 10:\n",
    "                        cluster_center_init.append(df[df[ass] == np.sort(df[df[ass] >= high][ass])[sum(df[ass] >= high)//2]].iloc[0, :])\n",
    "                        sit = True\n",
    "                    elif ass_25 - low > 20 or sum(df[ass] <= low) >= 10:\n",
    "                        cluster_center_init.append(df[df[ass] == np.sort(df[df[ass] <= low][ass])[sum(df[ass] <= low)//2]].iloc[0, :])\n",
    "                        sit = True\n",
    "                    if sit == True:\n",
    "                        break\n",
    "                if sit == False:\n",
    "                    cluster_center_init.append(cur_cluster_center)\n",
    "            cluster_center_init = np.array(cluster_center_init)\n",
    "            return np.unique(cluster_center_init, axis=0)\n",
    "\n",
    "        #Merge clusters with very similar cluster centers\n",
    "        def merge_cluster(feature, label, k, identical_asset=3, argsort=False):\n",
    "            clust_centers = []\n",
    "            for i in range(k):\n",
    "                similar = False\n",
    "                df = feature[label == i]\n",
    "                center = df.median(axis=0)\n",
    "                for c in clust_centers:\n",
    "                    if argsort:\n",
    "                        criteria = (sum((center - c).abs() <= 10) >= identical_asset and max((center - c).abs()) <= 20) or \\\n",
    "                    (all(np.argsort(np.floor(center)) == np.argsort(np.floor(c))) and all((center - c).abs() <= 25))\n",
    "                    else:\n",
    "                        criteria = sum((center - c).abs() <= 10) >= identical_asset and max((center - c).abs()) <= 20\n",
    "\n",
    "                    if criteria:\n",
    "                        similar = True\n",
    "                        break\n",
    "                if not similar:\n",
    "                    clust_centers.append(center)\n",
    "            clust_centers = np.array(clust_centers)\n",
    "            return np.unique(clust_centers, axis=0)\n",
    "\n",
    "        #Look for cluster groups with less than 10 components and check if they can be merged into other clusters\n",
    "        def merge_outlier(label, data_nostd, log=False):\n",
    "            n = len(np.unique(label))\n",
    "            centers = {}\n",
    "            for i in range(n):\n",
    "                centers[i] = data_nostd[label==i].median(axis=0)\n",
    "\n",
    "            for i in range(n):\n",
    "                if sum(label == i) <= 10:\n",
    "                    mn = float('inf')\n",
    "                    res = None\n",
    "                    for j in range(n):\n",
    "                        if j == i:\n",
    "                            continue\n",
    "                        if j in centers:\n",
    "                            distance = (centers[i] - centers[j]).abs().sum()\n",
    "                        else:\n",
    "                            continue\n",
    "                        if all(np.argsort(np.floor(centers[i])) == np.argsort(np.floor(centers[j]))) and distance < mn:\n",
    "                            mn = distance\n",
    "                            res = j\n",
    "                    if res is not None:\n",
    "                        label = np.where(label == i, res, label)\n",
    "                        del centers[i]\n",
    "                        if log:\n",
    "                            print(f'cluster{i} get merged into cluster{res}')\n",
    "\n",
    "            new_n = len(np.unique(label))\n",
    "            numbers = sorted(np.unique(label), reverse=True)\n",
    "            l = 0\n",
    "            for i in range(new_n):\n",
    "                if i not in numbers:\n",
    "                    label = np.where(label == numbers[l], i, label)\n",
    "                    l += 1\n",
    "            return label\n",
    "\n",
    "        #Plot the boxplot distribution of holding assets of each cluster\n",
    "        def plot_asset(asset, label, k, asset_type):\n",
    "            width = 20\n",
    "            height = ((k//10)+1)*15\n",
    "            plt.figure(figsize=(width, height))\n",
    "            for i in range(1, k+1):\n",
    "                plt.subplot(int(np.ceil(k/4)), 4, i)\n",
    "                data = [asset.loc[asset.index[label == i-1], :][col] for col in asset_type]\n",
    "                plt.boxplot(data)\n",
    "                plt.xlabel(asset_type)\n",
    "                plt.title(f'cluster_{i-1} with {sum(label==i-1)} components')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        #Non function part\n",
    "        k = get_silhouette(self.feature, log)\n",
    "        h_clustering = cluster.AgglomerativeClustering(n_clusters=k, linkage='ward').fit(self.feature)\n",
    "        cluster_label = h_clustering.labels_\n",
    "        print('Best number of clusters for hierarchical clustering is', k)\n",
    "\n",
    "        #Screen out the outliers\n",
    "        length1 = -1\n",
    "        length2 = -2\n",
    "        print('Starting searching and grouping outliers')\n",
    "        #Look for the outliers\n",
    "        while length1 != length2:\n",
    "            cluster_center_init = get_new_center(self.feature_nostd, cluster_label, k)\n",
    "            length1 = k = len(cluster_center_init)\n",
    "            clustering = cluster.KMeans(n_clusters=k, init=cluster_center_init, n_init=1).fit(self.feature_nostd)\n",
    "            cluster_label = clustering.labels_\n",
    "\n",
    "            cluster_center_init = get_new_center(self.feature_nostd, cluster_label, k)\n",
    "            length2 = k = len(cluster_center_init)\n",
    "            clustering = cluster.KMeans(n_clusters=k, init=cluster_center_init, n_init=1).fit(self.feature_nostd)\n",
    "            cluster_label = clustering.labels_\n",
    "            if log:\n",
    "                print('split outliers:', length1, length2)\n",
    "        print('Split finished. No more outliers could be found in the cluster according to the given criteria.')\n",
    "\n",
    "\n",
    "        #Merge cluster centers that are very close\n",
    "        length1 = -1\n",
    "        length2 = -2\n",
    "        while length1 != length2:\n",
    "            new_cluster_center_init = merge_cluster(self.feature_nostd, cluster_label, k, identical_asset, argsort)\n",
    "            new_cluster_center_init = new_cluster_center_init[~np.isnan(new_cluster_center_init).any(axis=1)]\n",
    "            length1 = k = len(new_cluster_center_init)\n",
    "            clustering = cluster.KMeans(n_clusters=k, init=new_cluster_center_init, n_init=1).fit(self.feature_nostd)\n",
    "            cluster_label = clustering.labels_\n",
    "\n",
    "            new_cluster_center_init = merge_cluster(self.feature_nostd, cluster_label, k, identical_asset, argsort)\n",
    "            new_cluster_center_init = new_cluster_center_init[~np.isnan(new_cluster_center_init).any(axis=1)]\n",
    "            length2 = k = len(new_cluster_center_init)\n",
    "            clustering = cluster.KMeans(n_clusters=k, init=new_cluster_center_init, n_init=1).fit(self.feature_nostd)\n",
    "            cluster_label = clustering.labels_\n",
    "            if log:\n",
    "                print('converge centers:', length1, length2)\n",
    "        print('Merging finished. No more centroids could be merged cross different clusters according to the given criteria.')\n",
    "\n",
    "        print('Start to merge outlier clusters.')\n",
    "        cluster_label = merge_outlier(cluster_label, self.feature_nostd, log)\n",
    "        self.label = cluster_label\n",
    "        k = len(np.unique(cluster_label))\n",
    "        self.k = k\n",
    "        print('The final clusters are', k)\n",
    "\n",
    "        if log:\n",
    "            plot_asset(self.feature_nostd, cluster_label, k, self.asset_type)\n",
    "        return cluster_label\n",
    "    \n",
    "    def write_csv(self, fundno_ticker, loc='final_output'):\n",
    "        def label_cluster(l, data_nostd):\n",
    "            k = len(np.unique(l))\n",
    "            labelling = {}\n",
    "            ref = {0:data_nostd.columns[0], \n",
    "                   1:data_nostd.columns[1],\n",
    "                   2:data_nostd.columns[2],\n",
    "                   3:data_nostd.columns[3]}\n",
    "            for i in range(k):\n",
    "                index = data_nostd[l==i].median(axis=0)\n",
    "                if index[0] <= -20:\n",
    "                    if index[1] >= 80:\n",
    "                        labelling[i] = 'Leveraged ' + ref[1] + ' group'\n",
    "                    elif index[2] >= 80:\n",
    "                        labelling[i] = 'Leveraged ' + ref[2] + ' group'\n",
    "                    elif index[3] >= 80:\n",
    "                        labelling[i] = 'Leveraged ' + ref[3] + ' group'\n",
    "                    else:\n",
    "                        labelling[i] = 'Leveraged mixed group'\n",
    "                elif max(index) >= 80:\n",
    "                    labelling[i] = np.argmax(index) + ' driven group'\n",
    "                elif sum(list(map(lambda x: x >= 10, index))) >= 3:\n",
    "                    labelling[i] = 'Deversified group'\n",
    "                elif sum(list(map(lambda x: x >= 25, index))) >= 2:\n",
    "                    if index[0] >= 25 and index[1] >= 25:\n",
    "                        labelling[i] = ref[0] + ' ' + ref[1] + ' mixed group'\n",
    "                    elif index[0] >= 25 and index[2] >= 25:\n",
    "                        labelling[i] = ref[0] + ' ' + ref[2] + ' mixed group'\n",
    "                    elif index[0] >= 25 and index[3] >= 25:\n",
    "                        labelling[i] = ref[0] + ' ' + ref[3] + ' mixed group'\n",
    "                    elif index[1] >= 25 and index[2] >= 25:\n",
    "                        labelling[i] = ref[1] + ' ' + ref[2] + ' mixed group'\n",
    "                    elif index[1] >= 25 and index[3] >= 25:\n",
    "                        labelling[i] = ref[1] + ' ' + ref[3] + ' mixed group'\n",
    "                    elif index[2] >= 25 and index[3] >= 25:\n",
    "                        labelling[i] = ref[2] + ' ' + ref[3] + ' mixed group'\n",
    "                else:\n",
    "                    labelling[i] = 'Mixed investment group'\n",
    "            return labelling\n",
    "        \n",
    "        def get_sharpe_ret(cumret_df, ret_df, cluster_label):\n",
    "            if cumret_df.shape[1] != ret_df.shape[1] != len(cluster_label):\n",
    "                raise ValueError('The input data are not consistent')\n",
    "\n",
    "            sharpe_dict = {}\n",
    "            absolute_return = {}\n",
    "            for i in range(len(np.unique(cluster_label))):\n",
    "                temp_dict = {}\n",
    "                df = cumret_df[cumret_df.columns[cluster_label == i]]\n",
    "                annual_return = (df.iloc[-1, :]) - 1\n",
    "                annual_vol = ret_df[ret_df.columns[cluster_label == i]].std(axis=0)*np.sqrt(250)\n",
    "                rf = 0.05 #assumption\n",
    "                annual_sharpe_ratio = (annual_return - rf)/annual_vol\n",
    "                pct_25 = np.round(annual_sharpe_ratio.describe()['25%'], 4)\n",
    "                pct_75 = np.round(annual_sharpe_ratio.describe()['75%'], 4)\n",
    "                low = np.round(pct_25 - (pct_75 - pct_25)*1.5, 4)\n",
    "                high = np.round(pct_75 + (pct_75 - pct_25)*1.5, 4)\n",
    "                for fund in annual_sharpe_ratio.index:\n",
    "                    if annual_sharpe_ratio[fund] >= pct_25 and pct_75 > annual_sharpe_ratio[fund]:\n",
    "                        sharpe_dict[fund] = f'medium: {pct_25}-{pct_75}'\n",
    "                    elif annual_sharpe_ratio[fund] >= pct_75 and high > annual_sharpe_ratio[fund]:\n",
    "                        sharpe_dict[fund] = f'high: {pct_75}-{high}'\n",
    "                    elif annual_sharpe_ratio[fund] >= low and pct_25 > annual_sharpe_ratio[fund]:\n",
    "                        sharpe_dict[fund] = f'low: {low}-{pct_25}'\n",
    "                    elif annual_sharpe_ratio[fund] >= high:\n",
    "                        sharpe_dict[fund] = f'very high: >{high}'\n",
    "                    elif annual_sharpe_ratio[fund] < low:\n",
    "                        sharpe_dict[fund] = f'very low: <{low}'\n",
    "\n",
    "                temp_dict['med'] = list(annual_sharpe_ratio.index[(pct_75 > annual_sharpe_ratio) & (annual_sharpe_ratio >= pct_25)])\n",
    "                temp_dict['high'] = list(annual_sharpe_ratio.index[(high > annual_sharpe_ratio) & (annual_sharpe_ratio >= pct_75)])\n",
    "                temp_dict['very high'] = list(annual_sharpe_ratio.index[annual_sharpe_ratio >= high])\n",
    "                temp_dict['low'] = list(annual_sharpe_ratio.index[(pct_25 > annual_sharpe_ratio) & (annual_sharpe_ratio >= low)])\n",
    "                temp_dict['very low'] = list(annual_sharpe_ratio.index[annual_sharpe_ratio < low])\n",
    "                for key in temp_dict.keys():\n",
    "                    temp_df = cumret_df[temp_dict[key]]\n",
    "                    final_return = (temp_df.iloc[-1, :]) - 1\n",
    "                    threshold = final_return.median()\n",
    "                    for j in final_return.index:\n",
    "                        if final_return[j] >= threshold:\n",
    "                            absolute_return[j] = ('high', np.round(final_return[j], 4))\n",
    "                        else:\n",
    "                            absolute_return[j] = ('low', np.round(final_return[j], 4))\n",
    "            return sharpe_dict, absolute_return\n",
    "        \n",
    "        \n",
    "        import os\n",
    "        if not os.path.exists(loc):\n",
    "            os.makedirs(loc)\n",
    "        \n",
    "        fund_ctgy = {self.mrnstar_data.iloc[j, :]['crsp_fundno']: self.mrnstar_data.iloc[j, :]['lipper_class_name'] for j in range(len(self.mrnstar_data))}\n",
    "        cluster_label = label_cluster(self.label, self.feature_nostd)\n",
    "        sharpe_rank, abret_rank = get_sharpe_ret(self.cumret_data, self.ret_data, self.label)\n",
    "        df = pd.DataFrame(columns=['Fund.No', 'Ticker', 'Cluster'] + self.asset_type + ['Mstar Category', \n",
    "                                   'Cluster Category', 'sharpe_ratio', 'absolute_return', 'absolute_return_val'])\n",
    "        df['Fund.No'] = np.array(self.feature_nostd.index)\n",
    "        df['Ticker'] = df['Fund.No'].apply(lambda x: fundno_ticker[int(x)])\n",
    "        df['Cluster'] = self.label\n",
    "        for ass in self.asset_type:\n",
    "            df[ass] = np.array(self.feature_nostd[ass])\n",
    "        df['Mstar Category'] = df['Fund.No'].apply(lambda x: fund_ctgy.get(int(x), ''))\n",
    "        df['Cluster Category'] = df['Cluster'].apply(lambda x: cluster_label[x])\n",
    "        df['sharpe_ratio'] = df['Fund.No'].apply(lambda x: sharpe_rank[str(x)])\n",
    "        df['absolute_return'] = df['Fund.No'].apply(lambda x: abret_rank[str(x)][0])\n",
    "        df['absolute_return_val'] = df['Fund.No'].apply(lambda x: abret_rank[str(x)][1])\n",
    "        df.to_csv(f'{loc}/cluster_result_{self.clustering_year}.csv', index=False)\n",
    "        print('Sucessfully saved the clustering output!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flc = first_layer_clustering(2019, data_trimmed, holding_asset, fund_mornstar)\n",
    "# feature, feature_nostd = flc.init_features()\n",
    "# label_2019 = flc.asset_based_clustering()\n",
    "# flc.write_csv(fundno_ticker, loc='final_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flc = first_layer_clustering(2019, data_trimmed, holding_asset, fund_mornstar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature, feature_nostd = flc.init_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_2019 = flc.asset_based_clustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flc.write_csv(fundno_ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stability check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stability(label1, label2, data1_nostd, data2_nostd, fund=False):\n",
    "    #fund controls whether to show a specific fund's cluster change in two different time periods\n",
    "    exceptions = []\n",
    "    change_explainable, change_nonexplainable = [], []\n",
    "    for i in range(len(label1)):\n",
    "        fund = data1_nostd.index[i]\n",
    "        if fund not in data2_nostd.index:\n",
    "            exceptions.append(int(fund))\n",
    "            continue\n",
    "\n",
    "        no_1 = label1[i]\n",
    "        no_2 = label2[list(data2_nostd.index).index(fund)]\n",
    "        cluster_member_1 = set(data1_nostd.index[label1 == no_1].astype(int)) - {int(fund)}\n",
    "        cluster_member_2 = set(data2_nostd.index[label2 == no_2].astype(int)) - {int(fund)}\n",
    "        group1_index = data1_nostd[label1==no_1].median()\n",
    "        group2_index = data2_nostd[label2==no_2].median()\n",
    "        \n",
    "        if len(cluster_member_1) == len(cluster_member_2) == 0:\n",
    "            change_explainable.append(fund)\n",
    "            continue\n",
    "        if len(cluster_member_1) == 0:\n",
    "            if all((data2_nostd.loc[fund, :] - group2_index) <= 10):\n",
    "                change_explainable.append(fund)\n",
    "                continue\n",
    "            else:\n",
    "                change_nonexplainable.append([fund])\n",
    "                continue\n",
    "        \n",
    "        member_change_proportion = len(cluster_member_1.intersection(cluster_member_2))/len(cluster_member_1)\n",
    "        if all((group1_index-group2_index).abs() <= 10) or member_change_proportion >= 0.7:\n",
    "            continue\n",
    "        if member_change_proportion < 0.7:\n",
    "            holding_1 = data1_nostd.loc[fund, :]\n",
    "            holding_2 = data2_nostd.loc[fund, :]\n",
    "            change = (holding_1 - holding_2).abs()\n",
    "            if all(change <= 10):\n",
    "                if (sum(label1 == no_1) < 30 or sum(label2 == no_2) < 30):\n",
    "                    change_explainable.append(fund)\n",
    "                elif all(data1_nostd.loc[fund, :] - group1_index <= 5) and all(data2_nostd.loc[fund, :] - group2_index <= 5):\n",
    "                    change_explainable.append(fund)\n",
    "                else:\n",
    "                    change_nonexplainable.append([fund, member_change_proportion])\n",
    "            else:\n",
    "                change_explainable.append(fund)\n",
    "    \n",
    "    if fund:\n",
    "        print('The fund is in cluster', label2[list(data2_nostd.index).index(fund)], 'in previous year')\n",
    "        print('The fund is in cluster', label1[list(data1_nostd.index).index(fund)], 'in later year')\n",
    "        print()\n",
    "        print('The asset info of the fund in previous year:')\n",
    "        print(data2_nostd.loc[fund,:])\n",
    "        print('The asset info of the fund in later year:')\n",
    "        print(data1_nostd.loc[fund,:])\n",
    "        print()\n",
    "        print('The median asset info of the cluster where the fund is in previous year is:')\n",
    "        print(data2_nostd[label2 == label2[list(data2_nostd.index).index(fund)]].median())\n",
    "        print('The median asset info of the cluster where the fund is in later year is:')\n",
    "        print(data1_nostd[label1 == label1[list(data1_nostd.index).index(fund)]].median())\n",
    "        print()\n",
    "    return change_explainable, change_nonexplainable, exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change_explainable, change_nonexplainable, exceptions = stability(label_2019, label_2018, features_2019_nostd, features_2018_nostd)\n",
    "# print(len(change_nonexplainable), len(change_explainable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second layer - subclustering based on result in first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tslearn\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import davies_bouldin_score, pairwise_distances\n",
    "from gap_statistic import OptimalK\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import argparse\n",
    "from time import time\n",
    "\n",
    "# Keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, UpSampling2D, Conv2DTranspose, GlobalAveragePooling1D, Softmax\n",
    "from keras.losses import kullback_leibler_divergence\n",
    "import keras.backend as K\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "\n",
    "# Dataset helper function\n",
    "from ipynb.fs.full.datasets import load_data\n",
    "\n",
    "# DTC components\n",
    "from ipynb.fs.full.TSClusteringLayer import TSClusteringLayer\n",
    "from ipynb.fs.full.TAE import temporal_autoencoder\n",
    "from ipynb.fs.full.metrics import *\n",
    "import ipynb.fs.full.tsdistances as tsdistances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class second_layer_clustering:\n",
    "    \n",
    "    def __init__(self, clustering_year, ret_data, asset_data, mrnstar_class_data, feature_first_layer, first_layer_result, group):\n",
    "        self.clustering_year = clustering_year\n",
    "        self.ret_data = ret_data[ret_data.index.year == clustering_year]\n",
    "        self.cumret_data = (self.ret_data+1).cumprod()\n",
    "        self.asset_data = asset_data[(asset_data.caldt.dt.year == clustering_year) & (asset_data.caldt.dt.month == 12)]\n",
    "        self.mrnstar_data = mrnstar_class_data[(mrnstar_class_data.caldt.dt.year == (clustering_year)) & (mrnstar_class_data.caldt.dt.month == 12)]\n",
    "        self.feature = feature_first_layer\n",
    "        self.label = first_layer_result\n",
    "        self.k = len(set(self.label))\n",
    "        self.first_layer_output_csv = None\n",
    "        if group >= self.k:\n",
    "            raise ValueError('Group out of range')\n",
    "        else:\n",
    "            self.group = group\n",
    "        \n",
    "    def get_timeseries(self, ret=False, val=False):\n",
    "        \"\"\"\n",
    "        ret: the daily return time series data of each fund\n",
    "        val: the cumulative return time series data of each fund\n",
    "        ret and val should at least provide one\n",
    "        year: the clustering year, should be consistent with the first layer\n",
    "        label: the clustering result from the first layer\n",
    "        group: the specific group in the label to sub-cluster on\n",
    "        feature: the feature data used in the first layer, to help match the fund number with the label\n",
    "        \"\"\"\n",
    "        from sklearn import preprocessing\n",
    "        if ret and val:\n",
    "            ret = self.ret_data\n",
    "            ret = ret[self.feature.index]\n",
    "            ret = ret.iloc[:, self.label==self.group]\n",
    "            val = (ret+1).cumprod()\n",
    "            fundnos = ret.columns\n",
    "            ret_train = ret.T\n",
    "            val_train = val.T\n",
    "            scaler1 = preprocessing.StandardScaler()\n",
    "            scaler2 = preprocessing.StandardScaler()\n",
    "            ret_train = scaler1.fit_transform(ret_train)\n",
    "            val_train = scaler2.fit_transform(val_train)\n",
    "\n",
    "            #reshape\n",
    "            array1 = np.array(ret_train).reshape([ret_train.shape[0], ret_train.shape[1], -1])\n",
    "            array2 = np.array(val_train).reshape([val_train.shape[0], val_train.shape[1], -1])\n",
    "            res_train = np.concatenate((array1, array2), axis = 2)\n",
    "            \n",
    "            self.new_feature = res_train\n",
    "            self.fundnos = fundnos\n",
    "            return res_train, fundnos\n",
    "\n",
    "        elif ret or val:\n",
    "            df = self.ret_data\n",
    "            df = df[self.feature.index]\n",
    "            df = df.iloc[:, self.label==self.group]\n",
    "            if ret:\n",
    "                pass\n",
    "            else:\n",
    "                df = (df+1).cumprod()\n",
    "            fundnos = df.columns\n",
    "            df_train = df.T\n",
    "            scaler1 = preprocessing.StandardScaler()\n",
    "            df_train = scaler1.fit_transform(df_train)\n",
    "\n",
    "            #reshape\n",
    "            res_train = np.array(df_train).reshape([df_train.shape[0], df_train.shape[1], -1])\n",
    "            \n",
    "            self.new_feature = res_train\n",
    "            self.fundnos = fundnos\n",
    "            return res_train, fundnos\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Should at least provide one')\n",
    "            \n",
    "    def organize_label(self, subcluster_label):\n",
    "        label = subcluster_label\n",
    "        label_set = set(label)\n",
    "        number_of_cluster = len(label_set)\n",
    "        for i in range(number_of_cluster):\n",
    "            if i in label_set:\n",
    "                continue\n",
    "            else:\n",
    "                mx = max(label_set)\n",
    "                label = np.where(label==mx, i, label)\n",
    "                label_set.add(i)\n",
    "                label_set.remove(mx)\n",
    "        self.subcluster_label = label\n",
    "        self.subcluster_k = len(set(label))\n",
    "        return label\n",
    "    \n",
    "    def plot_sub_result(self):\n",
    "        k = self.subcluster_k\n",
    "        width = 16\n",
    "        height = ((k//10)+1)*15\n",
    "        plt.figure(figsize=(width, height))\n",
    "        for i in range(1, k+1):\n",
    "            plt.subplot(int(np.ceil(k/4)), 4, i)\n",
    "            for fund in self.fundnos[self.subcluster_label==(i-1)]:\n",
    "                plt.plot(self.cumret_data[fund])\n",
    "            plt.title(f'Cluster_{i-1} with {sum(self.subcluster_label==(i-1))} components')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DTC Model -- first train the autoencoder to then train the clustering model\n",
    "class DTC:\n",
    "    \"\"\"\n",
    "    Deep Temporal Clustering (DTC) model\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters\n",
    "        input_dim: input dimensionality\n",
    "        timesteps: length of input sequences (can be None for variable length)\n",
    "        n_filters: number of filters in convolutional layer\n",
    "        kernel_size: size of kernel in convolutional layer\n",
    "        strides: strides in convolutional layer\n",
    "        pool_size: pooling size in max pooling layer, must divide the time series length\n",
    "        n_units: numbers of units in the two BiLSTM layers\n",
    "        alpha: coefficient in Student's kernel\n",
    "        dist_metric: distance metric between latent sequences\n",
    "        cluster_init: cluster initialization method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, input_dim, timesteps,\n",
    "                 n_filters=50, kernel_size=10, strides=1, pool_size=10, n_units=[50, 1],\n",
    "                 alpha=1.0, dist_metric='eucl', cluster_init='kmeans'):\n",
    "        assert(timesteps % pool_size == 0)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.input_dim = input_dim\n",
    "        self.timesteps = timesteps\n",
    "        self.n_filters = n_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.pool_size = pool_size\n",
    "        self.n_units = n_units\n",
    "        self.latent_shape = (self.timesteps // self.pool_size, self.n_units[1])\n",
    "        self.alpha = alpha\n",
    "        self.dist_metric = dist_metric\n",
    "        self.cluster_init = cluster_init\n",
    "        self.pretrained = False\n",
    "        self.model = self.autoencoder = self.encoder = self.decoder = None\n",
    "    \n",
    "    \n",
    "    #Autoencoder part\n",
    "    def initialize_autoencoder(self):\n",
    "        \"\"\"\n",
    "        Create DTC model\n",
    "        \"\"\"\n",
    "        # Create AE models\n",
    "        self.autoencoder, self.encoder, self.decoder = temporal_autoencoder(input_dim=self.input_dim,\n",
    "                                                                            timesteps=self.timesteps,\n",
    "                                                                            n_filters=self.n_filters,\n",
    "                                                                            kernel_size=self.kernel_size,\n",
    "                                                                            strides=self.strides,\n",
    "                                                                            pool_size=self.pool_size,\n",
    "                                                                            n_units=self.n_units)\n",
    "        \n",
    "    def load_ae_weights(self, ae_weights_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained weights of AE\n",
    "        # Arguments\n",
    "            ae_weight_path: path to weights file (.h5)\n",
    "        \"\"\"\n",
    "        self.autoencoder.load_weights(ae_weights_path)\n",
    "        self.pretrained = True\n",
    "        \n",
    "    def pretrain(self, X,\n",
    "                 optimizer='adam',\n",
    "                 epochs=500,\n",
    "                 batch_size=64,\n",
    "                 save_dir='results',\n",
    "                 verbose=1):\n",
    "        \"\"\"\n",
    "        Pre-train the autoencoder using only MSE reconstruction loss\n",
    "        Saves weights in h5 format.\n",
    "        # Arguments\n",
    "            X: training set\n",
    "            optimizer: optimization algorithm\n",
    "            epochs: number of pre-training epochs\n",
    "            batch_size: training batch size\n",
    "            save_dir: path to existing directory where weights will be saved\n",
    "        \"\"\"\n",
    "        print('Pretraining...')\n",
    "        self.autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        # Begin pretraining\n",
    "        t0 = time()\n",
    "        self.autoencoder.fit(X, X, batch_size=batch_size, epochs=epochs, verbose=verbose)\n",
    "        print('Pretraining time: ', time() - t0)\n",
    "        self.autoencoder.save_weights('{}/ae_weights-epoch{}.h5'.format(save_dir, epochs))\n",
    "        print('Pretrained weights are saved to {}/ae_weights-epoch{}.h5'.format(save_dir, epochs))\n",
    "        self.pretrained = True\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encoding function. Extract latent features from hidden layer\n",
    "        # Arguments\n",
    "            x: data point\n",
    "        # Return\n",
    "            encoded (latent) data point\n",
    "        \"\"\"\n",
    "        self.features = self.encoder.predict(x)\n",
    "        return self.features\n",
    "\n",
    "    def decode(self, x):\n",
    "        \"\"\"\n",
    "        Decoding function. Decodes encoded sequence from latent space.\n",
    "        # Arguments\n",
    "            x: encoded (latent) data point\n",
    "        # Return\n",
    "            decoded data point\n",
    "        \"\"\"\n",
    "        return self.decoder.predict(x)  \n",
    "    \n",
    "    #Clustering part\n",
    "    def compile_clustering_model(self, optimizer):\n",
    "        \"\"\"\n",
    "        Compile DTC model\n",
    "        # Arguments\n",
    "            gamma: coefficient of TS clustering loss\n",
    "            optimizer: optimization algorithm\n",
    "            initial_heatmap_loss_weight (optional): initial weight of heatmap loss vs clustering loss\n",
    "            final_heatmap_loss_weight (optional): final weight of heatmap loss vs clustering loss (heatmap finetuning)\n",
    "        \"\"\"\n",
    "        clustering_input = Input(shape=(self.features.shape[1], self.features.shape[2]), name='clustering_input')\n",
    "        \n",
    "        clustering_layer = TSClusteringLayer(self.n_clusters,\n",
    "                                             alpha=self.alpha,\n",
    "                                             dist_metric=self.dist_metric,\n",
    "                                             name='TSClustering')(clustering_input)\n",
    "        \n",
    "        self.model = Model(inputs=clustering_input,\n",
    "                           outputs=clustering_layer)\n",
    "        print(self.model.summary())\n",
    "        \n",
    "        self.model.compile(loss='kld',\n",
    "                           optimizer=optimizer)\n",
    "    \n",
    "    #Initialize cluster centers\n",
    "    def init_cluster_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize with complete-linkage hierarchical clustering or k-means.\n",
    "        # Arguments\n",
    "            X: numpy array containing training set or batch\n",
    "        \"\"\"\n",
    "        assert(self.cluster_init in ['hierarchical', 'kmeans'])\n",
    "        print('Initializing cluster...')\n",
    "        \n",
    "        features = self.features\n",
    "\n",
    "        if self.cluster_init == 'hierarchical':\n",
    "            if self.dist_metric == 'eucl':  # use AgglomerativeClustering off-the-shelf\n",
    "                hc = AgglomerativeClustering(n_clusters=self.n_clusters,\n",
    "                                             affinity='euclidean',\n",
    "                                             linkage='complete').fit(features.reshape(features.shape[0], -1))\n",
    "            else:  # compute distance matrix using dist\n",
    "                d = np.zeros((features.shape[0], features.shape[0]))\n",
    "                for i in range(features.shape[0]):\n",
    "                    for j in range(i):\n",
    "                        d[i, j] = d[j, i] = self.dist(features[i], features[j])\n",
    "                hc = AgglomerativeClustering(n_clusters=self.n_clusters,\n",
    "                                             affinity='precomputed',\n",
    "                                             linkage='complete').fit(d)\n",
    "            # compute centroid\n",
    "            cluster_centers = np.array([features[hc.labels_ == c].mean(axis=0) for c in range(self.n_clusters)])\n",
    "        elif self.cluster_init == 'kmeans':\n",
    "            # fit k-means on flattened features\n",
    "            km = KMeans(n_clusters=self.n_clusters, n_init=10).fit(features.reshape(features.shape[0], -1))\n",
    "            cluster_centers = km.cluster_centers_.reshape(self.n_clusters, features.shape[1], features.shape[2])\n",
    "\n",
    "        self.model.get_layer(name='TSClustering').set_weights([cluster_centers])\n",
    "        print('Done!')\n",
    "    \n",
    "    @property\n",
    "    def cluster_centers_(self):\n",
    "        \"\"\"\n",
    "        Returns cluster centers\n",
    "        \"\"\"\n",
    "        return self.model.get_layer(name='TSClustering').get_weights()[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def weighted_kld(loss_weight):\n",
    "        \"\"\"\n",
    "        Custom KL-divergence loss with a variable weight parameter\n",
    "        \"\"\"\n",
    "        def loss(y_true, y_pred):\n",
    "            return loss_weight * kullback_leibler_divergence(y_true, y_pred)\n",
    "        return loss\n",
    "\n",
    "    def load_weights(self, weights_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained weights of DTC model\n",
    "        # Arguments\n",
    "            weight_path: path to weights file (.h5)\n",
    "        \"\"\"\n",
    "        self.model.load_weights(weights_path)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def dist(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Compute distance between two multivariate time series using chosen distance metric\n",
    "        # Arguments\n",
    "            x1: first input (np array)\n",
    "            x2: second input (np array)\n",
    "        # Return\n",
    "            distance\n",
    "        \"\"\"\n",
    "        if self.dist_metric == 'eucl':\n",
    "            return tsdistances.eucl(x1, x2)\n",
    "        elif self.dist_metric == 'cid':\n",
    "            return tsdistances.cid(x1, x2)\n",
    "        elif self.dist_metric == 'cor':\n",
    "            return tsdistances.cor(x1, x2)\n",
    "        elif self.dist_metric == 'acf':\n",
    "            return tsdistances.acf(x1, x2)\n",
    "        else:\n",
    "            raise ValueError('Available distances are eucl, cid, cor and acf!')\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict cluster assignment.\n",
    "        \"\"\"\n",
    "        q = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):  # target distribution p which enhances the discrimination of soft label q\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def fit(self, X_train, y_train=None,\n",
    "            X_val=None, y_val=None,\n",
    "            epochs=100,\n",
    "            eval_epochs=10,\n",
    "            save_epochs=10,\n",
    "            batch_size=64,\n",
    "            tol=0.001,\n",
    "            patience=5,\n",
    "            save_dir='results_v2'):\n",
    "        \"\"\"\n",
    "        Training procedure\n",
    "        # Arguments\n",
    "           X_train: training set\n",
    "           y_train: (optional) training labels\n",
    "           X_val: (optional) validation set\n",
    "           y_val: (optional) validation labels\n",
    "           epochs: number of training epochs\n",
    "           eval_epochs: evaluate metrics on train/val set every eval_epochs epochs\n",
    "           save_epochs: save model weights every save_epochs epochs\n",
    "           batch_size: training batch size\n",
    "           tol: tolerance for stopping criterion\n",
    "           patience: patience for stopping criterion\n",
    "           save_dir: path to existing directory where weights and logs are saved\n",
    "        \"\"\"\n",
    "        if not self.pretrained:\n",
    "            print('Autoencoder was not pre-trained!')\n",
    "\n",
    "        # Logging file\n",
    "        logfile = open(save_dir + '/dtc_log.csv', 'w')\n",
    "        fieldnames = ['epoch', 'T', 'L', 'Lr', 'Lc']\n",
    "        if X_val is not None:\n",
    "            fieldnames += ['L_val', 'Lr_val', 'Lc_val']\n",
    "        if y_train is not None:\n",
    "            fieldnames += ['acc', 'pur', 'nmi', 'ari']\n",
    "        if y_val is not None:\n",
    "            fieldnames += ['acc_val', 'pur_val', 'nmi_val', 'ari_val']\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames)\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        y_pred_last = None\n",
    "        patience_cnt = 0\n",
    "\n",
    "        print('Training for {} epochs.\\nEvaluating every {} and saving model every {} epochs.'.format(epochs, eval_epochs, save_epochs))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # Compute cluster assignments for training set\n",
    "            q = self.model.predict(X_train)\n",
    "            p = DTC.target_distribution(q)\n",
    "\n",
    "            # Evaluate losses and metrics on training set\n",
    "            if epoch % eval_epochs == 0:\n",
    "\n",
    "                # Initialize log dictionary\n",
    "                logdict = dict(epoch=epoch)\n",
    "\n",
    "                y_pred = q.argmax(axis=1)\n",
    "                if X_val is not None:\n",
    "                    q_val = self.model.predict(X_val)\n",
    "                    p_val = DTC.target_distribution(q_val)\n",
    "                    y_val_pred = q_val.argmax(axis=1)\n",
    "\n",
    "                print('epoch {}'.format(epoch))\n",
    "                loss = self.model.evaluate(X_train, p, batch_size=batch_size, verbose=False)\n",
    "                logdict['L'] = loss\n",
    "                print('[Train] - total loss={:f}'.format(logdict['L']))\n",
    "                if X_val is not None:\n",
    "                    val_loss = self.model.evaluate(X_val, p_val, batch_size=batch_size, verbose=False)\n",
    "                    logdict['L_val'] = val_loss\n",
    "                    print('[Val] - total loss={:f}'.format(logdict['L_val']))\n",
    "\n",
    "                # Evaluate the clustering performance using labels\n",
    "                if y_train is not None:\n",
    "                    logdict['acc'] = cluster_acc(y_train, y_pred)\n",
    "                    logdict['pur'] = cluster_purity(y_train, y_pred)\n",
    "                    logdict['nmi'] = metrics.normalized_mutual_info_score(y_train, y_pred)\n",
    "                    logdict['ari'] = metrics.adjusted_rand_score(y_train, y_pred)\n",
    "                    print('[Train] - Acc={:f}, Pur={:f}, NMI={:f}, ARI={:f}'.format(logdict['acc'], logdict['pur'],\n",
    "                                                                                    logdict['nmi'], logdict['ari']))\n",
    "                if y_val is not None:\n",
    "                    logdict['acc_val'] = cluster_acc(y_val, y_val_pred)\n",
    "                    logdict['pur_val'] = cluster_purity(y_val, y_val_pred)\n",
    "                    logdict['nmi_val'] = metrics.normalized_mutual_info_score(y_val, y_val_pred)\n",
    "                    logdict['ari_val'] = metrics.adjusted_rand_score(y_val, y_val_pred)\n",
    "                    print('[Val] - Acc={:f}, Pur={:f}, NMI={:f}, ARI={:f}'.format(logdict['acc_val'], logdict['pur_val'],\n",
    "                                                                                  logdict['nmi_val'], logdict['ari_val']))\n",
    "\n",
    "                logwriter.writerow(logdict)\n",
    "\n",
    "                # check stop criterion\n",
    "                if y_pred_last is not None:\n",
    "                    assignment_changes = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "                y_pred_last = y_pred\n",
    "                if epoch > 0 and assignment_changes < tol:\n",
    "                    patience_cnt += 1\n",
    "                    print('Assignment changes {} < {} tolerance threshold. Patience: {}/{}.'.format(assignment_changes, tol, patience_cnt, patience))\n",
    "                    if patience_cnt >= patience:\n",
    "                        print('Reached max patience. Stopping training.')\n",
    "                        logfile.close()\n",
    "                        break\n",
    "                else:\n",
    "                    patience_cnt = 0\n",
    "\n",
    "            # Save intermediate model and plots\n",
    "            if epoch % save_epochs == 0:\n",
    "                self.model.save_weights(save_dir + '/DTC_model_' + str(epoch) + '.h5')\n",
    "                print('Saved model to:', save_dir + '/DTC_model_' + str(epoch) + '.h5')\n",
    "\n",
    "            # Train for one epoch\n",
    "            self.model.fit(X_train, p, epochs=1, batch_size=batch_size, verbose=False)\n",
    "\n",
    "        # Save the final model\n",
    "        logfile.close()\n",
    "        print('Saving model to:', save_dir + '/DTC_model_final.h5')\n",
    "        self.model.save_weights(save_dir + '/DTC_model_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slc = second_layer_clustering(clustering_year=2019, ret_data=data_trimmed, asset_data=holding_asset, \n",
    "#                               mrnstar_class_data=fund_mornstar, feature_first_layer=feature_nostd, \n",
    "#                               first_layer_result=label_2019, group=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressed_data, fundnos = slc.get_timeseries(ret=True, val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='train', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# # parser.add_argument('--dataset', default='CBF', help='UCR/UEA univariate or multivariate dataset')\n",
    "# #parser.add_argument('--validation', default=False, type=bool, help='use train/validation split')\n",
    "# parser.add_argument('--ae_weights', default=None, help='pre-trained autoencoder weights')\n",
    "# parser.add_argument('--n_clusters', default=None, type=int, help='number of clusters')\n",
    "# parser.add_argument('--n_filters', default=50, type=int, help='number of filters in convolutional layer')\n",
    "# parser.add_argument('--kernel_size', default=10, type=int, help='size of kernel in convolutional layer')\n",
    "# parser.add_argument('--strides', default=1, type=int, help='strides in convolutional layer')\n",
    "# parser.add_argument('--pool_size', default=12, type=int, help='pooling size in max pooling layer') #Encoder output will be (21, 2)\n",
    "# parser.add_argument('--n_units', nargs=2, default=[50, 1], type=int, help='numbers of units in the BiLSTM layers')\n",
    "# parser.add_argument('--gamma', default=1.0, type=float, help='coefficient of clustering loss')\n",
    "# parser.add_argument('--alpha', default=1.0, type=float, help='coefficient in Student\\'s kernel')\n",
    "# parser.add_argument('--dist_metric', default='eucl', type=str, choices=['eucl', 'cid', 'cor', 'acf'], help='distance metric between latent sequences')\n",
    "# parser.add_argument('--cluster_init', default='hierarchical', type=str, choices=['kmeans', 'hierarchical'], help='cluster initialization method')\n",
    "# parser.add_argument('--pretrain_epochs', default=500, type=int)\n",
    "# parser.add_argument('--pretrain_optimizer', default='adam', type=str)\n",
    "# parser.add_argument('--epochs', default=1000, type=int)\n",
    "# parser.add_argument('--optimizer', default='adam', type=str)\n",
    "# parser.add_argument('--eval_epochs', default=20, type=int)\n",
    "# parser.add_argument('--save_epochs', default=50, type=int)\n",
    "# parser.add_argument('--batch_size', default=64, type=int)\n",
    "# parser.add_argument('--tol', default=0.001, type=float, help='tolerance for stopping criterion')\n",
    "# parser.add_argument('--patience', default=5, type=int, help='patience for stopping criterion')\n",
    "# parser.add_argument('--save_dir', default='result_secondlayer')\n",
    "# args = parser.parse_args(args=[])\n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(args.save_dir):\n",
    "#     os.makedirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.n_clusters = 15\n",
    "# dtc = DTC(n_clusters=args.n_clusters,\n",
    "#           input_dim=compressed_data.shape[-1],\n",
    "#           timesteps=compressed_data.shape[1],\n",
    "#           n_filters=args.n_filters,\n",
    "#           kernel_size=args.kernel_size,\n",
    "#           strides=args.strides,\n",
    "#           pool_size=args.pool_size,\n",
    "#           n_units=args.n_units,\n",
    "#           alpha=args.alpha,\n",
    "#           dist_metric=args.dist_metric,\n",
    "#           cluster_init=args.cluster_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtc.initialize_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if args.ae_weights is None and args.pretrain_epochs > 0:\n",
    "#     dtc.pretrain(X=compressed_data, optimizer=args.pretrain_optimizer,\n",
    "#                  epochs=args.pretrain_epochs, batch_size=args.batch_size,\n",
    "#                  save_dir=args.save_dir)\n",
    "# elif args.ae_weights is not None:\n",
    "#     dtc.load_ae_weights(args.ae_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# secondlayer_features = dtc.encode(compressed_data)\n",
    "# dtc.compile_clustering_model(optimizer=args.optimizer)\n",
    "# dtc.init_cluster_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t0 = time()\n",
    "# dtc.fit(secondlayer_features, None, None, None, args.epochs, args.eval_epochs, args.save_epochs, args.batch_size,\n",
    "#         args.tol, args.patience, args.save_dir)\n",
    "# print('Training time: ', (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_p = dtc.model.predict(secondlayer_features)\n",
    "# subcluster_label = pred_p.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subcluster_label = slc.organize_label(subcluster_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slc.plot_sub_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine the pool size\n",
    "def isPrime(n):\n",
    "    import math\n",
    "    if n > 1:\n",
    "        if n == 2:\n",
    "            return True\n",
    "        if n % 2 == 0:\n",
    "            return False\n",
    "        for x in range(3, int(math.sqrt(n) + 1), 2):\n",
    "            if n % x == 0:\n",
    "                return False\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code realization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='train', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# parser.add_argument('--dataset', default='CBF', help='UCR/UEA univariate or multivariate dataset')\n",
    "#parser.add_argument('--validation', default=False, type=bool, help='use train/validation split')\n",
    "parser.add_argument('--year', default=2019, type=int)\n",
    "parser.add_argument('--ae_weights', default=None, help='pre-trained autoencoder weights')\n",
    "parser.add_argument('--n_clusters', default=None, type=int, help='number of clusters')\n",
    "parser.add_argument('--n_filters', default=50, type=int, help='number of filters in convolutional layer')\n",
    "parser.add_argument('--kernel_size', default=10, type=int, help='size of kernel in convolutional layer')\n",
    "parser.add_argument('--strides', default=1, type=int, help='strides in convolutional layer')\n",
    "parser.add_argument('--pool_size', default=12, type=int, help='pooling size in max pooling layer') #Encoder output will be (21, 2)\n",
    "parser.add_argument('--n_units', nargs=2, default=[50, 1], type=int, help='numbers of units in the BiLSTM layers')\n",
    "parser.add_argument('--gamma', default=1.0, type=float, help='coefficient of clustering loss')\n",
    "parser.add_argument('--alpha', default=1.0, type=float, help='coefficient in Student\\'s kernel')\n",
    "parser.add_argument('--dist_metric', default='eucl', type=str, choices=['eucl', 'cid', 'cor', 'acf'], help='distance metric between latent sequences')\n",
    "parser.add_argument('--cluster_init', default='hierarchical', type=str, choices=['kmeans', 'hierarchical'], help='cluster initialization method')\n",
    "parser.add_argument('--pretrain_epochs', default=500, type=int)\n",
    "parser.add_argument('--pretrain_optimizer', default='adam', type=str)\n",
    "parser.add_argument('--epochs', default=1000, type=int)\n",
    "parser.add_argument('--optimizer', default='adam', type=str)\n",
    "parser.add_argument('--eval_epochs', default=20, type=int)\n",
    "parser.add_argument('--save_epochs', default=50, type=int)\n",
    "parser.add_argument('--batch_size', default=64, type=int)\n",
    "parser.add_argument('--tol', default=0.001, type=float, help='tolerance for stopping criterion')\n",
    "parser.add_argument('--patience', default=5, type=int, help='patience for stopping criterion')\n",
    "parser.add_argument('--save_dir', default='result_secondlayer')\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.year = 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>First layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flc = first_layer_clustering(clustering_year=args.year, ret_data=data_trimmed, asset_data=holding_asset, \n",
    "                             mrnstar_class_data=fund_mornstar)\n",
    "feature, feature_nostd = flc.init_features()\n",
    "firstlayer_label = flc.asset_based_clustering()\n",
    "flc.write_csv(fundno_ticker, loc='result_firstlayer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li> Second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create saving path\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir)\n",
    "\n",
    "subcluster_dict = dict()\n",
    "for group in range(len(set(firstlayer_label))):\n",
    "    slc = second_layer_clustering(clustering_year=args.year, ret_data=data_trimmed, asset_data=holding_asset, \n",
    "                                  mrnstar_class_data=fund_mornstar, feature_first_layer=feature_nostd, \n",
    "                                  first_layer_result=firstlayer_label, group=group)\n",
    "    compressed_data, fundnos = slc.get_timeseries(ret=True, val=True)\n",
    "    \n",
    "    #Check if the sample is bigger than 1\n",
    "    if compressed_data.shape[0] == 1:\n",
    "        subcluster_dict[fundnos[0]] = 0\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    #determine the pool size\n",
    "    if isPrime(compressed_data.shape[1]):\n",
    "        compressed_data = compressed_data[:, :compressed_data.shape[1]-1, :]\n",
    "    for i in range(10, compressed_data.shape[1]//2):\n",
    "        if compressed_data.shape[1]%i == 0:\n",
    "            args.pool_size = i\n",
    "            break\n",
    "    \n",
    "    #initialize model\n",
    "    args.n_clusters = min(15, sum(firstlayer_label==group)//2)\n",
    "    dtc = DTC(n_clusters=args.n_clusters,\n",
    "              input_dim=compressed_data.shape[-1],\n",
    "              timesteps=compressed_data.shape[1],\n",
    "              n_filters=args.n_filters,\n",
    "              kernel_size=args.kernel_size,\n",
    "              strides=args.strides,\n",
    "              pool_size=args.pool_size,\n",
    "              n_units=args.n_units,\n",
    "              alpha=args.alpha,\n",
    "              dist_metric=args.dist_metric,\n",
    "              cluster_init=args.cluster_init)\n",
    "\n",
    "    #Train autoencoder\n",
    "    dtc.initialize_autoencoder()\n",
    "    if args.ae_weights is None and args.pretrain_epochs > 0:\n",
    "        dtc.pretrain(X=compressed_data, optimizer=args.pretrain_optimizer,\n",
    "                     epochs=args.pretrain_epochs, batch_size=args.batch_size,\n",
    "                     save_dir=args.save_dir)\n",
    "    elif args.ae_weights is not None:\n",
    "        dtc.load_ae_weights(args.ae_weights)\n",
    "\n",
    "    #Fetch the compressed data/result from the autoencoder\n",
    "    secondlayer_features = dtc.encode(compressed_data)\n",
    "\n",
    "    #Compile model\n",
    "    dtc.compile_clustering_model(optimizer=args.optimizer)\n",
    "\n",
    "    #Apply hierarchical clustering to select initial cluster centers used in KMeans\n",
    "    dtc.init_cluster_weights()\n",
    "\n",
    "    #Train clustering algorithm by using KL divergence as loss\n",
    "    t0 = time()\n",
    "    dtc.fit(secondlayer_features, None, None, None, args.epochs, args.eval_epochs, args.save_epochs, args.batch_size,\n",
    "            args.tol, args.patience, args.save_dir)\n",
    "    print('Training time: ', (time() - t0))\n",
    "\n",
    "    #Get the clustering result\n",
    "    pred_p = dtc.model.predict(secondlayer_features)\n",
    "    subcluster_label = pred_p.argmax(axis=1)\n",
    "    subcluster_label = slc.organize_label(subcluster_label)\n",
    "\n",
    "    #Plot it out to check\n",
    "    # slc.plot_sub_result()\n",
    "\n",
    "    #Write into the dictionary\n",
    "    for i in range(len(fundnos)):\n",
    "        subcluster_dict[fundnos[i]] = subcluster_label[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_result(first_layer_result_path, save_loc, subcluster_dict, first_layer_label, year):\n",
    "        import os\n",
    "        if not os.path.exists(save_loc):\n",
    "            os.makedirs(save_loc)\n",
    "        df = pd.read_csv(first_layer_result_path)\n",
    "        if len(subcluster_dict) != len(first_layer_label):\n",
    "            raise ValueError('The subclustering for each cluster is not finished.')\n",
    "        df.insert(int(np.where(df.columns=='Cluster')[0][0]+1), 'Subcluster', np.ones(len(df)))\n",
    "        df['Subcluster'] = df['Fund.No'].apply(lambda x: subcluster_dict[str(x)])\n",
    "        df.to_csv(f'{save_loc}/cluster_result_withsub_{year}.csv', index=False)\n",
    "        print('Successfully saved the csv file.')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_result(first_layer_result_path=f'result_firstlayer/cluster_result_{args.year}.csv', \n",
    "              save_loc='result_final', subcluster_dict=subcluster_dict, \n",
    "              first_layer_label=firstlayer_label, year=args.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
